{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import sys\n",
    "import time\n",
    "import urllib\n",
    "from urllib.error import HTTPError, URLError\n",
    "\n",
    "import bs4\n",
    "import requests\n",
    "import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_source(page_url):\n",
    "    soup = bs4.BeautifulSoup(urllib.request.urlopen(page_url), \"lxml\")\n",
    "    title = soup.find(id=\"firstHeading\").text\n",
    "    tmp = soup.find(\"script\").text.split(\"proofreadpage_source_href\")\n",
    "\n",
    "    if (len(tmp) == 1):\n",
    "        return page_url, title\n",
    "\n",
    "    else:\n",
    "        page_url = \"https://fr.wikisource.org\" + tmp[1].split()[1].split('\"')[1]\n",
    "        page_url = page_url[:-1]\n",
    "        return page_url, title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_dups(file):\n",
    "    uniques = set()\n",
    "    with open(file, \"r\", encoding=\"utf8\") as f:\n",
    "        for link in tqdm.tqdm(f.readlines()):\n",
    "            page, _ = get_source(link)\n",
    "            uniques.add('/'.join(page.split('/')[4:]).strip())\n",
    "    with open(file, 'w+') as f:\n",
    "        for item in uniques:\n",
    "            f.write(\"https://fr.wikisource.org/wiki/%s\\n\" % item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pages_url(source_url):\n",
    "    pages = []\n",
    "    try:\n",
    "        soup = bs4.BeautifulSoup(urllib.request.urlopen(source_url), \"lxml\")\n",
    "    except URLError:\n",
    "        pages.append(\"Non scrappable\")\n",
    "        return pages\n",
    "    for a in soup.findAll('a', {'class':['prp-pagequality-3 quality3', 'prp-pagequality-4 quality4']}):\n",
    "        pages.append(\"https://fr.wikisource.org/\" + a['href'])\n",
    "    if not pages:\n",
    "        return [source_url]\n",
    "    return pages\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_content_page(url):\n",
    "    if url == \"Non scrappable\":\n",
    "        text = \"Non scrappable\"\n",
    "        return text\n",
    "    else:\n",
    "        res = urllib.request.urlopen(url)\n",
    "        wiki = bs4.BeautifulSoup(res, \"lxml\")\n",
    "        elems = wiki.select('p')\n",
    "        text = \"\"\n",
    "        for elem in elems:\n",
    "            text += re.sub(r\"\\n\", \" \", elem.getText())\n",
    "            text += \"\\n\"\n",
    "        text = re.sub(r\"(\\[\\d+\\])+\", \"\", text)\n",
    "        return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_content_pages(urls):\n",
    "    content = \"\"\n",
    "    pbar = tqdm.tqdm(urls)\n",
    "    soup = bs4.BeautifulSoup(urllib.request.urlopen(urls[0]), \"lxml\")\n",
    "    title = soup.find(id=\"firstHeading\").text\n",
    "    for url in pbar:\n",
    "        content += get_content_page(url) + \" \"\n",
    "        pbar.set_description(\"Processing %s\" % url.split('/')[-2])\n",
    "    return content, title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_to_file(text, title):\n",
    "    f = '\":.*<>|/\\\\?'\n",
    "    for char in f:\n",
    "        if char in title:\n",
    "            title = title.replace(char, \" \")\n",
    "    if not os.path.exists('Raw text Sciences'):\n",
    "        os.makedirs('Raw text Sciences')\n",
    "    with open(os.path.join('Raw text Sciences', title + \".txt\"), \"w\", encoding=\"utf8\") as file:\n",
    "        file.write(text)\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # You can comment out this first line if the URL list is clean already (no dups).\n",
    "    remove_dups(\"UrlListSciences.txt\")\n",
    "    f = open(\"UrlListSciences.txt\", \"r\")\n",
    "    lines = f.readlines()\n",
    "    f.close()\n",
    "    for line in lines:\n",
    "        livre, titre = get_content_pages(get_pages_url(line))\n",
    "        save_to_file(livre, titre)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
